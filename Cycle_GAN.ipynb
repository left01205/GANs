{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb17fb6",
   "metadata": {},
   "source": [
    "ResNet & PatchGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df50caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4626eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self , in_fts):\n",
    "        super(ResidualBlock , self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels = in_fts , out_channels = in_fts , kernel_size=3),\n",
    "            nn.InstanceNorm2d(in_fts),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels = in_fts , out_channels= in_fts , kernel_size = 3),\n",
    "            nn.InstanceNorm2d(in_fts)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward (self , x):\n",
    "        return x +self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e422c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks=6):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "        \n",
    "        # Initial Convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling: 64 -> 128 -> 256\n",
    "        in_features = 64\n",
    "        for _ in range(2):\n",
    "            out_features = in_features * 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual Blocks: stays at 256\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling: 256 -> 128 -> 64\n",
    "        # The error happened here. We must ensure in_features starts at 256.\n",
    "        for _ in range(2):\n",
    "            out_features = in_features // 2\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output Layer\n",
    "        model += [nn.ReflectionPad2d(3), nn.Conv2d(64, channels, 7), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa58bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self , input_shape , num_residual_blocs = 6):\n",
    "        super(GeneratorResNet , self ).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels = channels , out_channels = 64 , kernel_size = 7 ),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace = True)\n",
    "        ]\n",
    "\n",
    "        in_fts = 64\n",
    "\n",
    "        for _ in range(2):\n",
    "            \n",
    "            out_fts = in_fts * 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_channels = in_fts , out_channels = out_fts , kernel_size = 3 , stride = 2 , padding = 1 ),\n",
    "                nn.InstanceNorm2d(out_fts),\n",
    "                nn.ReLU(inplace = True)\n",
    "            ]\n",
    "            in_fts = out_fts\n",
    "             \n",
    "        for _ in range(num_residual_blocs):\n",
    "            model += [ResidualBlock(in_fts)]\n",
    "\n",
    "        for _ in range(2):\n",
    "            out_fts = in_fts //2\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_channels = in_fts , out_channels = out_fts , kernel_size = 3 , stride  = 2 , padding = 1 , output_padding = 1),\n",
    "                nn.InstanceNorm2d(out_fts),\n",
    "                nn.ReLU(inplace = True)\n",
    "            ] \n",
    "        model += [nn.ReflectionPad2d(3),\n",
    "                  nn.Conv2d(in_channels = 64 , out_channels = channels , kernel_size = 7),\n",
    "                  nn.Tanh()\n",
    "                  ] \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self , x):\n",
    "        return self.model(x)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5759e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self , input_shape):\n",
    "        super(Discriminator , self ).__init__()\n",
    "        channels , H , W = input_shape\n",
    "\n",
    "        def discriminator_block(in_filters , out_filters , normalize = True):\n",
    "            layers = [nn.Conv2d(in_channels = in_filters ,out_channels = out_filters , kernel_size  = 4 , stride = 2  , padding = 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2 , inplace = True))\n",
    "            return layers \n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels , 64 , normalize = False),\n",
    "            *discriminator_block(64 , 128),\n",
    "            *discriminator_block(128 , 256),\n",
    "            *discriminator_block(256 , 512),\n",
    "\n",
    "            nn.ZeroPad2d((1,0,1,0)),\n",
    "            nn.Conv2d(512 , 1 , 4, padding = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self , img):\n",
    "        return self.model(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28987c",
   "metadata": {},
   "source": [
    "Utilities (Buffer & Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self , max_size = 50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self , data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element , 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform( 0, 1 ) >0.5:\n",
    "                    i = random.randint(0 , self.max_size -1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)\n",
    "    \n",
    "\n",
    "def save_sample(epoch , batch_i , real_A , real_B , G_AB , G_BA):\n",
    "    G_AB.eval() ; G_BA.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_B , fake_A = G_AB(real_A) , G_BA(real_B)\n",
    "        recov_A , recov_B = G_BA(fake_B) , G_AB(fake_A)\n",
    "\n",
    "        grid = torch.cat((real_A[:4] , fake_B[:4], recov_A[:4] , real_B[:4] , fake_A[:4] , recov_B[:4]) , 0 )\n",
    "        os.makedirs(\"output_img_Cycle_GAN\", exist_ok=True)\n",
    "        vutils.save_image((grid + 1.0) / 2, f\"output_img_Cycle_GAN/output_epoch_{epoch}.png\", nrow=4)\n",
    "    G_AB.train() ; G_BA.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3febed",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612787b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = (3 , 32 , 32)\n",
    "\n",
    "G_AB = GeneratorResNet(input_shape).to(device)\n",
    "G_BA = GeneratorResNet(input_shape).to(device)\n",
    "D_A = Discriminator(input_shape).to(device)\n",
    "D_B = Discriminator(input_shape).to(device)\n",
    "\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "opt_G = torch.optim.Adam(list(G_AB.parameters()) + list(G_BA.parameters()) , lr = 0.0002 , betas = (0.5 , 0.999))\n",
    "opt_DA = torch.optim.Adam(D_A.parameters() , lr = 0.0002 , betas = (0.5,0.999))\n",
    "opt_DB = torch.optim.Adam(D_B.parameters() , lr = 0.0002 , betas = (0.5 , 0.999))\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "\n",
    "def train(dataloader , epochs = 200):\n",
    "    for epoch in range(epochs):\n",
    "        for i , batch in enumerate(dataloader):\n",
    "\n",
    "            real_A = batch['A'].to(device)\n",
    "            real_B = batch['B'].to(device)\n",
    "\n",
    "            valid = torch.ones((real_A.size(0) , 1 , 2 , 2)).to(device)\n",
    "            fake = torch.zeros((real_A.size(0) , 1 , 2 , 2)).to(device)\n",
    "\n",
    "            opt_G.zero_grad()\n",
    "            loss_id = (criterion_identity(G_BA(real_A) , real_A) + criterion_identity(G_AB(real_B) , real_B)) /2\n",
    "\n",
    "            fake_B , fake_A = G_AB(real_A) , G_AB(real_B)\n",
    "            loss_GAN = (criterion_GAN(D_B(fake_B) , valid) + criterion_cycle(G_AB(fake_A), real_B)) /2\n",
    "            loss_cycle = (criterion_cycle(G_BA(fake_B) , real_A) + criterion_cycle(G_AB(fake_A) , real_B)) /2\n",
    "\n",
    "            loss_G = loss_GAN + (10.0 * loss_cycle) + (5.0 * loss_id)\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "            opt_DA.zero_grad()\n",
    "            f_A_buff = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_DA = (criterion_GAN(D_A(real_A), valid) + criterion_GAN(D_A(f_A_buff.detach()) , fake)) /2\n",
    "            loss_DA.backward();opt_DB.step()\n",
    "\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Batch{i} | Loss D_A: {loss_DA.item()} | Loss G: {loss_G.item()}\")\n",
    "                save_sample(epoch , i , real_A , real_B , G_AB , G_BA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7f66c",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d51414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class UnpairedDataset(Dataset):\n",
    "    def __init__(self, data_A, data_B, transform=None):\n",
    "        self.transform = transform\n",
    "        self.files_A = data_A\n",
    "        self.files_B = data_B\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = self.files_A[index % len(self.files_A)]\n",
    "\n",
    "        image_B = self.files_B[random.randint(0, len(self.files_B) - 1)]\n",
    "\n",
    "        if self.transform:\n",
    "            image_A = self.transform(image_A)\n",
    "            image_B = self.transform(image_B)\n",
    "\n",
    "        return {\"A\": image_A, \"B\": image_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dbc29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets , transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5 , 0.5 , 0.5) , (0.5 ,0.5 , 0.5))\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "full_ds =  datasets.CIFAR10(root = './data' , download = True , transform = transform)\n",
    "data_A = [img for img , label in full_ds if label ==0]\n",
    "data_B = [img for img , label in full_ds if label ==1]\n",
    "\n",
    "dataset = UnpairedDataset(data_A , data_B , transform = None)\n",
    "dataloader = DataLoader(dataset , batch_size = 4 , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b23be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch0 | Loss D_A: 0.7149485349655151 | Loss G: 8.337106704711914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(dataloader, epochs)\u001b[39m\n\u001b[32m     36\u001b[39m loss_cycle = (criterion_cycle(G_BA(fake_B) , real_A) + criterion_cycle(G_AB(fake_A) , real_B)) /\u001b[32m2\u001b[39m\n\u001b[32m     38\u001b[39m loss_G = loss_GAN + (\u001b[32m10.0\u001b[39m * loss_cycle) + (\u001b[32m5.0\u001b[39m * loss_id)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mloss_G\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m opt_G.step()\n\u001b[32m     42\u001b[39m opt_DA.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(dataloader , epochs = 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
